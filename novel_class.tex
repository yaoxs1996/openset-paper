\documentclass[11pt]{article}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\pdfminorversion=4
	% NOTE: To produce blinded version, replace "0" with "1" below.
	\newcommand{\blind}{0}
	
	%%%%%%% IISE Transactions margin specifications %%%%%%%%%%%%%%%%%%%
	% DON'T change margins - should be 1 inch all around.
	\addtolength{\oddsidemargin}{-.5in}%
	\addtolength{\evensidemargin}{-.5in}%
	\addtolength{\textwidth}{1in}%
	\addtolength{\textheight}{1.3in}%
	\addtolength{\topmargin}{-.8in}%
    \makeatletter
    \renewcommand\section{\@startsection {section}{1}{\z@}%
                                       {-3.5ex \@plus -1ex \@minus -.2ex}%
                                       {2.3ex \@plus.2ex}%
                                       {\normalfont\fontfamily{phv}\fontsize{16}{19}\bfseries}}
    \renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
                                         {-3.25ex\@plus -1ex \@minus -.2ex}%
                                         {1.5ex \@plus .2ex}%
                                         {\normalfont\fontfamily{phv}\fontsize{14}{17}\bfseries}}
    \renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                                        {-3.25ex\@plus -1ex \@minus -.2ex}%
                                         {1.5ex \@plus .2ex}%
                                         {\normalfont\normalsize\fontfamily{phv}\fontsize{14}{17}\selectfont}}
    \makeatother
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%%%%% IISE Transactions package list %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\usepackage{amsmath}
	\usepackage{graphicx}
	\usepackage{enumerate}
	\usepackage{natbib} %comment out if you do not have the package
	\usepackage{url} % not crucial - just used below for the URL
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%%%%% Author package list and commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%% Here are some examples %%%%%%%%%%%%%%
	%	\usepackage{amsfonts, amsthm, latexsym, amssymb}
	%	\usepackage{lineno}
	%	\newcommand{\mb}{\mathbf}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\begin{document}
		
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\def\spacingset#1{\renewcommand{\baselinestretch}%
			{#1}\small\normalsize} \spacingset{1}
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		\if0\blind
		{
			\title{\bf Novel Class Detection Based on Autoencoder and Micro Clusters}
			\author{Xiaoshun Yao $^a$ and Jane Roe $^b$ \\
			$^a$ Department, University, City, Country \\
             $^b$ Department, University, City, Country }
			\date{}
			\maketitle
		} \fi
		
		\if1\blind
		{

            \title{\bf \emph{IISE Transactions} \LaTeX \ Template}
			\author{Author information is purposely removed for double-blind review}
			
\bigskip
			\bigskip
			\bigskip
			\begin{center}
				{\LARGE\bf \emph{IISE Transactions} \LaTeX \ Template}
			\end{center}
			\medskip
		} \fi
		\bigskip
		
	\begin{abstract}
Learning how to distinguish novel classes data in new data is challenging for models due to the different distribution of novel classes data. Current existing methods often concentrate on finding a separatrix to discriminate known samples and novel samples or learning the distribution of the border of known samples so that fitting the novel classes samples. However, most of these works ignored that in high dimension feature space, even for the samples came from the same class, the distance between them may be greater than we thought, this also known as the curse of dimensionality. In this scene, current methods may get in stuck, especially when the input of models is original and high dimension data. To this end, we propose a novel classes detection method which based on denoising autoencoder and Micro-Clusters (N-AEMC). Using denoising autoencoder as a low dimension representation learning component, and at the same time, the median low dimension data served as the input of a classify network, the classifier try to classify known data correctly. By minimizing the sum of the loss of classifier and the loss of autoencoder, we make sure the median low dimension representation data can be classified in feature space and can be reconstructed the original input data. Then the median data will be sent into a Micro-Clusters framework, and formed several micro clusters and their statistical information. When new data arrived, they will pass the autoencoder, enter the Micro-Clusters framework and be discriminated whether belongs to novel classes. Our N-AEMC model tested in several data set and compared to state-of-the-art algorithms. 
	\end{abstract}
			
	\noindent%
	{\it Keywords:} \emph{Open set recognition}; \LaTeX; Manuscript format; Taylor \& Francis.

	%\newpage
	\spacingset{1.5} % DON'T change the spacing!


\section{Introduction} \label{s:intro}
In real world, for recognition tasks, due to various restriction, it is usually hard to collect all classes which are covered by training samples when training a classifier. So that in realistic scenario, relative tasks are facing a open set data, i.e. classifier may meet unknown classes in testing phase or realistic application scenario. For model, incomplete knowledge of the data at training time, and data from new classes can be submitted to model during testing, not only requiring the classifier model to accurately the seen classes data, but also need to deal with the unseen classes effectively. In the perspective of modeling forms, exist methods can be categorized into discriminative model and generative model. More in depth, from the discriminative model perspective, there are traditional-based methods and Deep Neural Network-based methods. As for generative model, there are Instance-based methods and Non-Instance generation-based method.

% 第二章：相关工作
\section{Related Work} \label{s:sec2}
The review of literature can be a separate section but it can be merged with the introduction section. One can find a number of reference examples at the end of this template, like,


% 第三章：预备工作
\section{Background} \label{s:methods}
We begin the brief of our Open Set Recognition framework with three concepts, including OSR problem formalization, autoencoder and dynamic micro clusters.

\subsection{OSR formalization} \label{s:methods.1}
OSR problem was first formalization in \cite{scheirer2012toward}, openness was used to assess the difficulty of particular problem by taking the classes number in training set, target set and testing set:
\begin{equation}
    openness = 1 - \sqrt{\frac{2\times |\text{training classes}|}{|\text{testing classes}|+ |\text{target classes}|}}
\end{equation}

Larger openness indicates more open problems i.e. more difficult. More straightforward to explain openness, when openness = 0, means all classes in testing set, training set has already contained, in this case, the problem is equivalent to normal recognition problem. The upper limit of openness is 1.0, which means all classes of testing set are never seen in training phase, but this scene in reality may be meaningless.



\subsubsection{\emph{Sub-subsection heading 3.1.1}} \label{s:methods.1.1}

Third-level headings should be in italics.
	
\subsection{\emph{Subsection heading 3.2}} \label{s:methods.2}


\subsection{\emph{Subsection heading 3.3}} \label{s:methods.3}

% 第4章：提出算法	
\section{Proposed algorithm} \label{s:numerical}
In this section, we will introduce the full detail of proposed algorithm.

\subsection{Overview}
The basic idea of our N-AEMC framework is learning the low dimension representation of original input data, and in the low dimension feature space, the known data will form several micro clusters and corresponding statistics information. When new data arrived, they will be sent into the autoencoder in order to get representation data and will be discriminated in the Micro-Clusters framework according to the location, i.e. when the new data in the known data clusters, it can be recognized as known data, otherwise, it may belong to novel classes data or outliers.

Our framework includes a low-dimension representation learning component and a novel classes discriminator. In the following, we will introduce each component one by one.

\subsection{Low-dimension representation learner}
The first component of our N-AEMC model is a low-dimension representation learning module, and this module contains two parts: a denoising autoencoder and a classify neural network.

We use a denoising autoencoder as a median representation learning module

% 第5章：实验

% 第6章：结论
\section{Conclusion}\label{s:conclusion}
A paper ends with a conclusion or summary section.

\if0\blind{
\section*{Acknowledgements}
The authors acknowledge the generous support from the funding agency of XYZ.	} \fi

\bibliographystyle{chicago}
\spacingset{1}
\bibliography{IISE-Trans}
	
\end{document}
